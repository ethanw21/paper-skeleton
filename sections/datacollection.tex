\section{Data Collection}\label{sec:datacollection}
We used the python library Twarc in order to download tweets from twitter's api. The dataset \cite{DBLP:journals/corr/abs-2101-05919} of Tweet IDs is a single csv file with one Tweet ID per line. Twarc's ‘hydrate’ command takes in a list of Tweet IDs and returns a json file containing all of the downloaded tweets, so we used this with the published dataset. Once we obtained this tweet data, we converted the json into a dataframe using the python library pandas. We discovered that while several countries were censoring tweets, many only had one or two blocked tweets in the dataset. We decided to limit our project to include tweets censored in France, Germany, India, Russia, and Turkey, seeing as they all had at least several thousand samples. We also found that a significant portion of the data consisted of tweets that were not written in English. This posed an issue for our machine learned models, which were trained only on English datasets. We decided to remove the non-English tweets to account for this. Limiting our data to only include English tweets that were censored in these five countries brought our total tweet count down from approximately 583,000 to 49,000.

\par

In order to collect a random sample of Twitter data to compare to our censored tweets, we again used Twarc. First, we grouped the tweets by the country in which they were censored. Then, we counted the number of censored tweets per country, and attempted to request a random sample of the same number of tweets from 2012 to 2020 from Twitter. Unfortunately, Twitter's built-in historical sampling feature is restricted by API access level, and we were thus unable to use it. To circumvent this, we then attempted to generate random timestamps between 2012 and 2020 and search for tweets published at those times. Unfortunately, we were again limited by API access level, rendering us unable to search for tweets based upon these randomly generated timestamps. We decided to use Twarc's ‘sample’ command to collect our tweets. This command listens to Twitter's streaming endpoint and gets a random sample of tweets being published in real time. This enabled us to gather a random sample of tweets to match each country’s respective censored tweet count in our dataset, but since it used Twitter's streaming endpoint, all of the tweets were recently published, so they did not fall in the same time period as our censored tweets. This is potentially a confounding variable when we compare the censored tweets to the uncensored tweets for a given country: we would like to attribute any major differences in the sentiment and topics between these two categories to the country’s censorship, but they may also be attributable to different topics and sentiments that people are truly expressing at various points in time. We ran this collection process on March 15th at 12:00 AM CST, so all of our random samples were tweets published around that time.

